Running HALE
============

Usage
-----
Install the hale package: :code:`pip install hale`

In the same environment, run the initialization script (in a standard 1-thread, 10-GB qlogin): :code:`python hale/scripts/run_initialization.py`.
This script accepts the following arguments:

--conn_def
    A connection definition to the GBD database. This connection definition must have SELECT permission on all databases on ADDRESS as well as the following permissions on the `gbd` database: INSERT, UPDATE, DELETE, EXECUTE, DROP, FILE, CREATE. Defaults to "gbd"

--population_run_id
    The mortality run ID with which to pull populations. If not specified, HALE uses the best run ID associated with the passed GBD round and decomp step.

--life_table_run_id
    The mortality run ID with which to pull life tables. If not specified, HALE uses the best run ID associated with the passed GBD round and decomp step.

--como_version
    The version of COMO with which to pull YLDs. If not specified, HALE uses the best COMO version associated with the passed GBD round and decomp step.

--gbd_round_id
    The GBD round on which to run HALE. Defaults to current GBD round.

--decomp_step
    The decomp step for which to run HALE.

--location_set_ids
    The location set IDs containing locations for which HALE should run. Typically only one location set ID should be passed. These locations must align with the locations in the life tables and YLDs.

--year_ids
    The year IDs for which to run HALE. Default 1980-2019.

--draws
    The number of draws at which to run HALE. This is limited to the number of draws used in the COMO version.

After initializing the HALE run, run the launch script (in a standard qlogin or by using qsub): :code:`python hale/scripts/run_launch.py`
This script accepts the following arguments:

--hale_version
    This is the HALE version that was created during initialization. It was printed during initialization, and it is also available in the `gbd.gbd.process_version` table. Note that for HALE, HALE version and GBD process version are one and the same.

--project
    The cluster project to use.

--error_dir
    Where to write stderr. Defaults to FILEPATH

--output_dir
    Where to write stdout. Defaults to FILEPATH

HALE is pretty quick, so this won't take long to run. When the workflow finishes, jobmon will print whether the worfklow executed succesfully or whether it failed.

Workflow
--------
HALE has four phases: initialization, HALE calculation, upload, and clean. 

Initialization
>>>>>>>>>>>>>>
Prepares data and tables needed to run HALE. This involves pulling life tables, YLDs, and population versions; creating a new GBD process version; setting up output directories; saving run metadata; and pulling and caching population.

HALE Calculation
>>>>>>>>>>>>>>>>
There is one HALE calculation job for each location. Each job spawns two processes to read life tables and YLDS. Pulling life tables involves:

- Reading the life table draws for a location.
- Reshaping those draws from long to wide.
- Downsampling (if HALE is run at <1000 draws)
- Replacing the 95-99 age group with 95+

Pulling YLDs involves:

- Reading the cached population
- Using :code:`get_draws` to read COMO draws for a location
- Converting to count space
- Aggregating neonatal/birth age groups
- Converting back to rate space

Next, the job calculates HALE by:

- Using Tx instead of nLx for age group 95+
- Adjusting Lx to nLx * (1 - YLDs)
- Calculating adjusted Tx by summing adjusted Lx for prior age groups
- Calculating HALE as adjusted Tx / Lx
- Copying HALE for age group 28 to age group 22
- Saving HALE draws by location and year

.. note::
    This excerpt from the DALYs/HALE paper summarizes the HALE calculation method:

    Average health values were incorporated into the life table by Sullivan’s method. First, we multiplied values in the Lx column of the life table—showing the years lived within an age interval starting at age x—by the corresponding average health value in that interval. The rest of the life table was then recalculated on the basis of the modiﬁed nLx values. In eﬀect, Sullivan’s method begins with an adjusted estimate of healthy life years lived within an interval, and subsequent life table calculations produce estimates of the total healthy life years remaining for all individuals alive at the start of each age interval (ie, those surviving to age x), and ﬁnally, the average remaining healthy life years at age x. Calculation of healthy life expectancy incorporated uncertainty in all three data inputs, generated by draws from uncertainty distributions around the component variables.

Finally, the job saves summaries for a location. Summaries include mean, upper, lower, and percent change.

Upload
>>>>>>
There is one upload job per HALE run. This job reads in HALE summaries and infiles them to the `gbd` database.

Clean
>>>>>
There is one clean job per HALE run. This job deletes temporary files (cached populations and CSVs used for infiling).

Inputs
------
Life tables (in long format) are loaded from :code:`FILEPATH/{run_id}/life_tables/lt_{location}.csv`. Note that HALE uses mortality process ID 28 (with shock death number) even though HALE pulls
inputs for process ID 29 (with shock life table). The draws for processes 28 and 29 are created as part of the
same pipeline, and both "with shock life table" and "with shock death number" draws are stored in the
"with shock death number" draw files. The "with shock life table" data isn't matched to process 29 until
means are uploaded to the database, so HALE uses process 28 because that process contains the life table draws even though the files are labeled as death number.

YLDs (in wide format) are pulled by location using :code:`get_draws`.

Populations are pulled using :code:`db_queries.get_population`.

Outputs
-------
The base HALE output directory is :code:`FILEPATH/v{version}`

HALE draws are saved in wide format by location and year to :code:`FILEPATH/v{version}/draws/results/{location}_{year}_draws.csv`. Each DataFrame contains the following columns: :code:`age_group_id`, :code:`location_id`, :code:`sex_id`, :code:`year_id`, :code:`cause_id`, :code:`draw_{i}`

HALE summaries are saved by location to :code:`FILEPATH/v{version}/summaries/results/{location}_summary.csv`. Each DataFrame contains the following columns: :code:`age_group_id`, :code:`location_id`, :code:`sex_id`, :code:`year_id`, :code:`HALE_lower`, :code:`HALE_mean`, and :code:`HALE_upper`

HALE summaries are also written to the `gbd` database. Single-year HALE is written to `gbd.output_hale_single_year_v{version}`, and percent change is written to `gbd.output_hale_multi_year_v{version}`

Intermediate Files
------------------
Population is cached by location to :code:`FILEPATH/v{version}/population/{location}_population.feather`

CSVs used for infiling are saved to :code:`FILEPATH/v{version}/output_hale_single_year.csv` and :code:`FILEPATH/v{version}/output_hale_multi_year.csv`

Run Metadata
------------
Population run ID, life table run ID, and COMO version are saved to the `gbd` database. In addition, the following metadata is saved to :code:`FILEPATH/v{version}/metadata.json`:

- HALE version
- GBD round ID
- Decomp step ID
- Location set version ID(s)
- Location IDs
- Year IDs
- Draws
- Population run ID
- Life table run ID
- COMO version
